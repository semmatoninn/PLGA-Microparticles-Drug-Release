{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/semmatoninn/PLGA-Microparticles-Drug-Release/blob/main/PLGAmicroparticle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Data Preparation"
      ],
      "metadata": {
        "id": "XjhSQS51iwfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 1147"
      ],
      "metadata": {
        "id": "mmgQe8b5i-xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "!pip install rdkit\n",
        "from rdkit import Chem # module chem\n",
        "from rdkit.Chem import Draw, Descriptors\n",
        "from rdkit.Chem import PandasTools # module from rdkit.Chem\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    max_error\n",
        ")\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import GridSearchCV, KFold, GroupKFold, RandomizedSearchCV\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "!pip install shap\n",
        "import shap"
      ],
      "metadata": {
        "id": "v2Pp-uDbr1ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/semmatoninn/PLGA-Microparticles-Drug-Release.git\n",
        "%cd PLGA-Microparticles-Drug-Release\n",
        "import pandas as pd\n",
        "df = pd.read_excel(\"mp_dataset_initial.xlsx\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "yZQnoMDvYmev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "d0ErlA8dvtn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "id": "K8Fpukd8Zj_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info() # 17 columns"
      ],
      "metadata": {
        "id": "u95C_3jtv4gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "eCob_zs5hFdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "8EfLPg3wv9Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropping Source link Column - No predictive value**"
      ],
      "metadata": {
        "id": "gBBDjbxCy9Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns = 'DOI')\n",
        "print(f\"After dropping columns: the number of columns are {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "iBQiNY_FxMzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Null Values**"
      ],
      "metadata": {
        "id": "vqSt8V_fA74X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of nulls in df: {df.isna().sum().sum()}\")"
      ],
      "metadata": {
        "id": "0zkCZ64Z_IEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "v6-hD8p9wd07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with too many missing values - 4 columns dropped\n",
        "df = df.drop(columns=['PDI', \"Polymer Mn\",\"Polymer Mw \",\"Polymer Molecular Weight\"])\n",
        "print(f\"After dropping columns: the number of columns are {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "OQGIpVmSHzH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplicates"
      ],
      "metadata": {
        "id": "3PXj-37GzXt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of duplicated rows: {df.duplicated().sum()}\")\n",
        "df.drop_duplicates(inplace=True) # dropping duplicates"
      ],
      "metadata": {
        "id": "z8cm0IKLyV5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Zero variance columns**"
      ],
      "metadata": {
        "id": "_NorK4QUA3qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_var_cols = df.columns[df.nunique() == 1].tolist()\n",
        "print(\"Zero-variance columns:\", zero_var_cols)\n"
      ],
      "metadata": {
        "id": "DU7m3n_nA0Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "8K7KvbUnz3QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(11)"
      ],
      "metadata": {
        "id": "0uE8ThIGz9i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Canonicalization**"
      ],
      "metadata": {
        "id": "U37I5Uw_OHA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def canonicalize_smiles(smiles, isomeric:bool=True): # isomeric is flag/true or false, true is default\n",
        "    \"\"\"\n",
        "    Convert any SMILES to its canonical form.\n",
        "\n",
        "    Args:\n",
        "        smiles (str): Input SMILES string\n",
        "\n",
        "    Returns:\n",
        "        str: Canonical SMILES\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles) # creating mol object\n",
        "    if mol is None:\n",
        "        return None  # Invalid SMILES\n",
        "    return Chem.MolToSmiles(mol, isomericSmiles=isomeric)"
      ],
      "metadata": {
        "id": "bdWhso6DOcUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Drug canonical_smiles'] = df['Drug SMILES'].apply(canonicalize_smiles)"
      ],
      "metadata": {
        "id": "DDazxPDKOK3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "zYMa9KhoOGrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column called 'molecule' with mol objects\n",
        "PandasTools.AddMoleculeColumnToFrame(df, # AddMoleculeColumnToFrame is a function inside PandasTools module\n",
        "                                     smilesCol='Drug canonical_smiles', # input column\n",
        "                                     molCol='Drug molecule' # output column - new column created\n",
        "                                     ) # creates mol object from canonical smiles column\n",
        "\n",
        "# df['molecule'] = df['canonical_smiles'].apply(Chem.MolFromSmiles) # the previous line is same as this\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rVj27jKIPX6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate key molecular descriptors\n",
        "def calculate_key_descriptors(mol):\n",
        "    \"\"\"Calculate key molecular descriptors for a molecule.\"\"\"\n",
        "\n",
        "    descriptors = {\n",
        "        'Drug MW': Descriptors.MolWt(mol),\n",
        "        'Drug LogP': Descriptors.MolLogP(mol), # Lipophilicity (octanol-water partition coefficient)\n",
        "        'Drug TPSA': Descriptors.TPSA(mol), # Topological Polar Surface Area (important for drug absorption)\n",
        "        # 'Drug HBD': Descriptors.NumHDonors(mol),\n",
        "        # 'Drug HBA': Descriptors.NumHAcceptors(mol)\n",
        "    }\n",
        "\n",
        "    return descriptors\n",
        "\n",
        "# Apply the function to the molecules in the DataFrame\n",
        "descriptor_data = []\n",
        "for idx, mol in df['Drug molecule'].items(): # index is row index, mol is molecule column value, molecule is mol object, it is pd series, index value pair\n",
        "    desc = calculate_key_descriptors(mol) # dictionary created for each sample molecule\n",
        "    descriptor_data.append(desc)\n",
        "\n",
        "# Create descriptor DataFrame with pd.DataFrame()\n",
        "# We use the same index as the original DataFrame to maintain row alignment\n",
        "descriptor_df = pd.DataFrame(descriptor_data, index=df.index) # new dataframe created with descriptor columns\n",
        "\n",
        "# Combine with the original molecules DataFrame\n",
        "df_with_descriptors = pd.concat([df, descriptor_df], axis=1) # axis = 1, side by side concatenation\n",
        "\n",
        "# Check the new dimensions - we should have the same number of rows but more columns\n",
        "print(f\"Dataset shape: {df_with_descriptors.shape}\")\n",
        "df_with_descriptors.head()"
      ],
      "metadata": {
        "id": "rrjWik2NPvPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train test split"
      ],
      "metadata": {
        "id": "YBpqgxjQL4GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "5bjJ5JUWyU1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode Formulation Method\n",
        "TARGET = \"Release \"\n",
        "df_encoded = pd.get_dummies(\n",
        "    df_with_descriptors,\n",
        "    columns=[\"Formulation Method\"],\n",
        "    drop_first=True\n",
        ")\n",
        "\n",
        "# Define FEATURES (exclude target + string/RDKit fields)\n",
        "exclude_cols = [\n",
        "    TARGET,\n",
        "    \"Drug\",\n",
        "    \"Drug SMILES\",\n",
        "    \"Drug canonical_smiles\",\n",
        "    \"Drug molecule\",\n",
        "    \"Formulation Index\"\n",
        "]\n",
        "\n",
        "FEATURES = [col for col in df_encoded.columns if col not in exclude_cols]\n",
        "\n",
        "# Group-aware train/test split (no leakage)\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "groups = df_encoded[\"Formulation Index\"]\n",
        "\n",
        "gss = GroupShuffleSplit(test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "train_idx, test_idx = next(gss.split(df_encoded, groups=groups))\n",
        "\n",
        "#  Final datasets\n",
        "X_train = df_encoded.iloc[train_idx][FEATURES]\n",
        "X_test  = df_encoded.iloc[test_idx][FEATURES]\n",
        "\n",
        "y_train = df_encoded.iloc[train_idx][TARGET]\n",
        "y_test  = df_encoded.iloc[test_idx][TARGET]\n",
        "\n",
        "# Looking at shapes\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_test:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "BnxEQYNjVXZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation analysis"
      ],
      "metadata": {
        "id": "JAajZdsxCnt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson"
      ],
      "metadata": {
        "id": "K2ZMjAAHs9S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_corr = X_train.copy()\n",
        "df_corr[\"Release \"] = y_train.values  # Add target column\n",
        "\n",
        "# Compute correlation\n",
        "corr_train = df_corr.corr(method=\"pearson\")\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "\n",
        "# Mask upper triangle for cleaner plot\n",
        "mask = np.triu(np.ones_like(corr_train, dtype=bool))\n",
        "\n",
        "sns.heatmap(\n",
        "    corr_train,\n",
        "    mask=mask,\n",
        "    cmap='coolwarm',\n",
        "    annot=True,   # Set True to show numbers\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=0.4,\n",
        "    cbar_kws={'label': 'Correlation Coefficient'}\n",
        ")\n",
        "\n",
        "plt.title(\"Correlation Heatmap (X_train + Release)\", fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "s5QTMNXWoN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Pearson correlation - linear correlations are captured\n",
        "\n",
        "Highest correlations between features are in\n",
        "\n",
        "Drug loading capacity - Initial drug to polymer ratio 0.88\n",
        "\n",
        "Drug TPSA and MW 0.87\n",
        "\n",
        "Drug Encapsulation Efficiency and Drug Loading Capacity 0.46\n",
        "\n",
        "Highest correlation with target variable release is time 0.42\n",
        "\n"
      ],
      "metadata": {
        "id": "_0nbE9hIM36y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spearman"
      ],
      "metadata": {
        "id": "vjamFQo9L7m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute Spearman correlation\n",
        "corr_train = df_corr.corr(method=\"spearman\")\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "\n",
        "# Mask upper triangle for cleaner plot\n",
        "mask = np.triu(np.ones_like(corr_train, dtype=bool))\n",
        "\n",
        "sns.heatmap(\n",
        "    corr_train,\n",
        "    mask=mask,\n",
        "    cmap='coolwarm',\n",
        "    annot=True,   # Set True to show numbers\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=0.4,\n",
        "    cbar_kws={'label': 'Correlation Coefficient'}\n",
        ")\n",
        "\n",
        "plt.title(\"Correlation Heatmap (X_train + Release)\", fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AAxZkE36pRlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Spearman correlations non-linear correlations are also captured, monotonic relationships are captured\n",
        "\n",
        "Highest correlated features are\n",
        "\n",
        "Drug loading capacity and initial drug to polymer ratio id 0.91\n",
        "\n",
        "Drug TPSA and MW correlation is 0.62\n",
        "\n",
        "Drug encapsulation efficiency and drug loading capacity correlation is 0.49\n",
        "\n",
        "Drug LogP and Solubility Enhancer Concentration correlation is 0.43\n",
        "\n",
        "Release and time correlation is 0.67\n"
      ],
      "metadata": {
        "id": "soxdda3cOQ7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Outliers"
      ],
      "metadata": {
        "id": "IkaDRKlC7QXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(X_train[numeric_cols], orient='h')\n",
        "# plt.tight_layout()\n",
        "plt.savefig(\"outlier_detection.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w7H57zFCq90t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## regular split\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(\n",
        "#    df[FEATURES], df[TARGET], test_size=0.2, random_state=RANDOM_SEED\n",
        "#)"
      ],
      "metadata": {
        "id": "gwsPbOV0Hj8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Modelling"
      ],
      "metadata": {
        "id": "rZOXiu0g7gbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ],
      "metadata": {
        "id": "rMb-XDgWUI7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_regression_metrics(model, X, y_true):\n",
        "    \"\"\"\n",
        "    Computes stable regression metrics for drug-release prediction.\n",
        "    MAPE is intentionally excluded because y contains zeros - it explodes.\n",
        "    \"\"\"\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Core metrics\n",
        "    r2  = r2_score(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mxe = max_error(y_true, y_pred)\n",
        "\n",
        "    return {\n",
        "        \"r2\": r2,\n",
        "        \"mse\": mse,\n",
        "        \"mae\": mae,\n",
        "        \"max_error\": mxe\n",
        "    }\n"
      ],
      "metadata": {
        "id": "tthT63tLKrHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression - Baseline"
      ],
      "metadata": {
        "id": "2-Zzwv0JI2S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "linear_regressor = LinearRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "linear_regressor.fit(X_train,y_train)\n",
        "\n",
        "linear_regressor_results_train = get_regression_metrics(linear_regressor,X_train,y_train )\n",
        "linear_regressor_results_test = get_regression_metrics(linear_regressor, X_test,y_test )\n",
        "\n",
        "# Print the results\n",
        "print(\"Linear Regression Results - Train Set:\", linear_regressor_results_train)\n",
        "print(\"Linear Regression Results - Test Set:\", linear_regressor_results_test)"
      ],
      "metadata": {
        "id": "rtCb2k6B4lMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression with Correlation based feature selection and scaler optimization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rKINYRUocvnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Correlation-based feature filter\n",
        "\n",
        "class CorrelationFilter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.9):\n",
        "        self.threshold = threshold\n",
        "        self.to_drop_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
        "\n",
        "        # Pearson absolute correlation\n",
        "        corr = X_df.corr().abs()\n",
        "\n",
        "        # Upper triangle (no diag)\n",
        "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "\n",
        "        # Choose features to drop\n",
        "        self.to_drop_ = [\n",
        "            column\n",
        "            for column in upper.columns\n",
        "            if any(upper[column] > self.threshold)\n",
        "        ]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
        "        return X_df.drop(columns=self.to_drop_, errors=\"ignore\")\n",
        "\n",
        "\n",
        "#\n",
        "# Pipeline: Corr Filter , Scaler , Linear Regression\n",
        "#    (scaler will be chosen via GridSearch)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"corr_filter\", CorrelationFilter()),\n",
        "    (\"scaler\", StandardScaler()),   # placeholder, will be overridden by param_grid\n",
        "    (\"model\", LinearRegression())\n",
        "])\n",
        "\n",
        "#  Hyperparameter grid\n",
        "#    correlation threshold\n",
        "#    scaler type: Standard, MinMax, or no scaling\n",
        "\n",
        "param_grid = {\n",
        "    \"corr_filter__threshold\": [0.75, 0.8, 0.85, 0.9, 0.95],\n",
        "    \"scaler\": [\n",
        "        StandardScaler(),\n",
        "        MinMaxScaler(),\n",
        "        \"passthrough\"   # no scaling\n",
        "    ]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"r2\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "#  Fit on training data\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV R²:\", grid.best_score_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "linear_corr_train = get_regression_metrics(best_model, X_train, y_train)\n",
        "linear_corr_test  = get_regression_metrics(best_model, X_test, y_test)\n",
        "\n",
        "print(\"Corr-Filtered + Scaler + Linear Regression - Train:\", linear_corr_train)\n",
        "print(\"Corr-Filtered + Scaler + Linear Regression - Test :\", linear_corr_test)\n",
        "\n",
        "best_filter = best_model.named_steps[\"corr_filter\"]\n",
        "print(\"Dropped features at best threshold:\", best_filter.to_drop_)\n",
        "\n",
        "print(\"Best scaler:\", best_model.named_steps[\"scaler\"])\n"
      ],
      "metadata": {
        "id": "gIk_eIQCExco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance increased very slightly"
      ],
      "metadata": {
        "id": "xVrwgXUZeOZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression with Forward feature selection and scaler optimization\n",
        "\n"
      ],
      "metadata": {
        "id": "HUB8fnHI3_e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base estimator\n",
        "base_model = LinearRegression()\n",
        "\n",
        "# Function to generate a forward selector with a given number of features\n",
        "def make_forward_selector(k):\n",
        "    return SequentialFeatureSelector(\n",
        "        estimator=base_model,\n",
        "        n_features_to_select=k,\n",
        "        direction=\"forward\",\n",
        "        scoring=\"r2\",\n",
        "        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),      # placeholder (GridSearch will override)\n",
        "    (\"feature_select\", make_forward_selector(5)),  # placeholder\n",
        "    (\"model\", LinearRegression()),\n",
        "])\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    \"scaler\": [\n",
        "        StandardScaler(),\n",
        "        MinMaxScaler(),\n",
        "        \"passthrough\"              # no scaling\n",
        "    ],\n",
        "    \"feature_select\": [\n",
        "        make_forward_selector(5),\n",
        "        make_forward_selector(8),\n",
        "        make_forward_selector(10),\n",
        "        make_forward_selector(12)\n",
        "    ],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"r2\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the full optimized pipeline\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model and parameters\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best CV R²:\", grid.best_score_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "\n",
        "train_metrics = get_regression_metrics(best_model, X_train, y_train)\n",
        "test_metrics  = get_regression_metrics(best_model, X_test, y_test)\n",
        "\n",
        "print(\"\\nOptimized Forward Selection + Scaler Pipeline\")\n",
        "print(\"Train:\", train_metrics)\n",
        "print(\"Test :\", test_metrics)\n",
        "\n",
        "\n",
        "sfs = best_model.named_steps[\"feature_select\"]\n",
        "selected_features = X_train.columns[sfs.get_support()]\n",
        "\n",
        "print(\"\\nSelected features:\")\n",
        "for f in selected_features:\n",
        "    print(\"  -\", f)\n"
      ],
      "metadata": {
        "id": "vFYkNFM4GzNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did not perform better"
      ],
      "metadata": {
        "id": "rCUdmnN44VnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KRR"
      ],
      "metadata": {
        "id": "fM6RQDNv7noH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "krr = KernelRidge(kernel='rbf') # without regularization and no scaling\n",
        "krr.fit(X_train,y_train)\n",
        "# get the metrics on the train and the test set using the get_regression_metrics functions (as above)\n",
        "krr_results_train = get_regression_metrics(krr, X_train,y_train )\n",
        "krr_results_test = get_regression_metrics(krr, X_test,y_test )\n",
        "\n",
        "\n",
        "# the results\n",
        "print(\"KRR Results - Train Set:\", krr_results_train)\n",
        "print(\"KRR Results - Test Set:\", krr_results_test)"
      ],
      "metadata": {
        "id": "KZ0A8ti27qp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "without regularization, KRR usually overfits, shown here as well"
      ],
      "metadata": {
        "id": "fORowae1HzYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_w_scaling = Pipeline(\n",
        "   [\n",
        "       ('scaling', StandardScaler()),\n",
        "       ('krr', KernelRidge(kernel=\"rbf\"))\n",
        "   ]\n",
        ")\n",
        "pipe_w_scaling.fit(X_train,y_train)\n",
        "\n",
        "pipeline_results_train = get_regression_metrics(pipe_w_scaling, X_train, y_train )\n",
        "pipeline_results_test = get_regression_metrics(pipe_w_scaling, X_test,y_test )\n",
        "\n",
        "\n",
        "# # Print the results\n",
        "print(\"Pipeline Results - Train Set:\", pipeline_results_train)\n",
        "print(\"Pipeline Results - Test Set:\", pipeline_results_test)"
      ],
      "metadata": {
        "id": "-iArbnj08Cm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipe_w_scaling.named_steps[\"krr\"].get_params())"
      ],
      "metadata": {
        "id": "jAbzn8XLoOg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mild overfitting for standardized default krr model"
      ],
      "metadata": {
        "id": "dkFdI5Cl8guY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KRR with Regularization and Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "2tOQUluZF3ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# possible scalers\n",
        "scalers = [StandardScaler(), MinMaxScaler(), \"passthrough\"]\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaling', StandardScaler()),\n",
        "    ('krr', KernelRidge(kernel='rbf'))\n",
        "])\n",
        "\n",
        "param_grid = [\n",
        "    {\n",
        "        'scaling': scalers,\n",
        "        'krr__alpha': [0.001, 0.01, 0.1, 1],\n",
        "        'krr__gamma': np.logspace(-3, 1, 20)\n",
        "    }\n",
        "]\n",
        "\n",
        "# Grid search with 5-fold CV\n",
        "search = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# Print best result\n",
        "print(\"Best Params:\", search.best_params_)\n",
        "print(\"Best CV R²:\", search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKht2gHuC0Y3",
        "outputId": "650ca5f9-dc2d-47b4-f033-e2c8e17f941e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.columns"
      ],
      "metadata": {
        "id": "KMsm-CExzFkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_krr = search.best_estimator_\n",
        "best_krr.fit(X_train, y_train)\n",
        "\n",
        "train_results = get_regression_metrics(best_krr, X_train, y_train)\n",
        "test_results  = get_regression_metrics(best_krr, X_test,  y_test)\n",
        "\n",
        "print(\"Tuned KRR - Train:\", train_results)\n",
        "print(\"Tuned KRR - Test :\", test_results)\n"
      ],
      "metadata": {
        "id": "eyVH73XXAwl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "better than linear regression models"
      ],
      "metadata": {
        "id": "KeLCRNiSh4Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# group aware cross validation\n",
        "\n",
        "# groups for TRAIN SET (must align with X_train)\n",
        "groups_train = df_encoded.iloc[train_idx][\"Formulation Index\"].values\n",
        "\n",
        "# Define pipeline placeholder (scaler will be set dynamically)\n",
        "pipe = Pipeline([\n",
        "    ('scaling', StandardScaler()),  # dummy; will be overridden in param_grid\n",
        "    ('krr', KernelRidge(kernel='rbf'))\n",
        "])\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'scaling': scalers,\n",
        "    'krr__alpha': [0.001, 0.01, 0.1, 1],\n",
        "    'krr__gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Group-aware CV instead of plain cv=5\n",
        "cv = GroupKFold(n_splits=5)\n",
        "\n",
        "# Grid search with group-aware CV\n",
        "search = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train, groups=groups_train)\n",
        "\n",
        "print(\"Best Params:\", search.best_params_)\n",
        "print(\"Best CV R²:\", search.best_score_)\n",
        "\n",
        "best_krr = search.best_estimator_\n",
        "best_krr.fit(X_train, y_train)\n",
        "\n",
        "train_results = get_regression_metrics(best_krr, X_train, y_train)\n",
        "test_results  = get_regression_metrics(best_krr, X_test,  y_test)\n",
        "\n",
        "print(\"Tuned KRR - Train:\", train_results)\n",
        "print(\"Tuned KRR - Test :\", test_results)"
      ],
      "metadata": {
        "id": "WNVq-dRR0kMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "8pCphzbnbQ2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost expects floats\n",
        "X_tr = X_train.to_numpy(dtype=float)\n",
        "y_tr = y_train.to_numpy(dtype=float)\n",
        "X_te = X_test.to_numpy(dtype=float)\n",
        "y_te = y_test.to_numpy(dtype=float)\n",
        "\n",
        "\n",
        "\n",
        "xgb = XGBRegressor(\n",
        "    n_estimators=1200,        # number of trees\n",
        "    learning_rate=0.04,       # how much each tree contributes\n",
        "    # max_depth=5,              # trees depth\n",
        "    # min_child_weight=12,      # larger leaves (regularization)\n",
        "    gamma=0.6,                # split penalty\n",
        "    # subsample=0.7,            # row sampling\n",
        "    # colsample_bytree=0.6,     # feature sampling\n",
        "    reg_alpha=1.0,            # L1\n",
        "    reg_lambda=6.0,           # L2\n",
        "    objective=\"reg:squarederror\",\n",
        "    tree_method=\"hist\",\n",
        "    n_jobs=4,\n",
        "    random_state=1147,\n",
        "    verbosity=0,\n",
        "    max_depth=4,\n",
        "    min_child_weight=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "\n",
        "xgb.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"XGBoost Results - Train Set:\", get_regression_metrics(xgb, X_tr, y_tr))\n",
        "print(\"XGBoost Results - Test Set:\",  get_regression_metrics(xgb, X_te, y_te))"
      ],
      "metadata": {
        "id": "fmSVisApbTIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xgboost performance shows that this is a difficult task to predict"
      ],
      "metadata": {
        "id": "S02gKzkHRcaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypertune XGBOOST"
      ],
      "metadata": {
        "id": "To__mCOU7WDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.model_selection import GroupKFold, RandomizedSearchCV  # or GridSearchCV\n",
        "# from sklearn.metrics import make_scorer, r2_score\n",
        "\n",
        "\n",
        "groups_train = df_encoded.iloc[train_idx][\"Formulation Index\"].values\n",
        "\n",
        "print(\"Unique formulation groups in train:\", np.unique(groups_train).shape[0])\n",
        "\n",
        "xgb = XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    tree_method=\"hist\",\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "param_distributions = {\n",
        "    \"n_estimators\": [300, 500, 800, 1200],\n",
        "    \"max_depth\": [3, 4, 5, 6],\n",
        "    \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
        "    \"subsample\": [0.6, 0.8, 1.0],\n",
        "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "    \"min_child_weight\": [1, 3, 5, 10],\n",
        "    \"gamma\": [0, 0.1, 0.3, 0.5],\n",
        "    \"reg_alpha\": [0, 0.01, 0.1, 1],\n",
        "    \"reg_lambda\": [0.1, 1, 5, 10],\n",
        "}\n",
        "\n",
        "# 4) Group-aware CV\n",
        "cv = GroupKFold(n_splits=5)\n",
        "\n",
        "# 5) RandomizedSearchCV (you can swap to GridSearchCV if you prefer)\n",
        "search_xgb = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,              # adjust depending on time\n",
        "    scoring=\"r2\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "search_xgb.fit(X_train, y_train, groups=groups_train)\n",
        "\n",
        "print(\"Best XGB Params:\", search_xgb.best_params_)\n",
        "print(\"Best XGB CV R²:\", search_xgb.best_score_)\n",
        "\n",
        "best_xgb = search_xgb.best_estimator_\n",
        "\n",
        "\n",
        "xgb_train_results = get_regression_metrics(best_xgb, X_train, y_train)\n",
        "xgb_test_results  = get_regression_metrics(best_xgb, X_test,  y_test)\n",
        "\n",
        "print(\"Tuned Group-Aware XGB - Train:\", xgb_train_results)\n",
        "print(\"Tuned Group-Aware XGB - Test :\", xgb_test_results)\n"
      ],
      "metadata": {
        "id": "FLaXviYM7yDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best model so far"
      ],
      "metadata": {
        "id": "qIO8YoC7kJxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III. Model Interpretation and Explainability"
      ],
      "metadata": {
        "id": "sEBRAlSUnQ_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PFI"
      ],
      "metadata": {
        "id": "N2uBdIi0kSSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_perm_importance(model, X, y, features, n=10):\n",
        "    \"\"\"\n",
        "    Calculate permutation importance score for each feature\n",
        "    \"\"\"\n",
        "    # Calculate baseline score (without permuting any feature)\n",
        "\n",
        "    baseline_score = model.score(X, y)\n",
        "\n",
        "    importance_scores = {}\n",
        "\n",
        "    # Loop over each feature\n",
        "    for feature in features:\n",
        "        X_perm = X.copy()\n",
        "        sum_score = 0\n",
        "\n",
        "        # Repeat n times to get average importance score\n",
        "        for i in range(n):\n",
        "            # Calculate score when given feature is permuted\n",
        "            X_perm[feature] = np.random.permutation(X_perm[feature].values)\n",
        "            permuted_score = model.score(X_perm, y)\n",
        "\n",
        "            sum_score += permuted_score\n",
        "\n",
        "        # Calculate decrease in score\n",
        "        importance_score = baseline_score - (sum_score / n)\n",
        "        importance_scores[feature] = importance_score\n",
        "\n",
        "    return importance_scores"
      ],
      "metadata": {
        "id": "SK1SXp7Xk2L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "features_no_time = [f for f in X_test.columns if \"time\" not in f.lower()]\n",
        "\n",
        "# Compute PFI for best_xgb\n",
        "\n",
        "print(\"Calculating permutation importance for best_xgb...\")\n",
        "\n",
        "xgb_perm_importance = get_perm_importance(\n",
        "    best_xgb,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    features_no_time        # time excluded\n",
        ")\n",
        "\n",
        "#  Sort and get top 10 features\n",
        "sorted_xgb_importance = sorted(\n",
        "    xgb_perm_importance.items(),\n",
        "    key=lambda item: item[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "top_10_xgb_features = [item[0] for item in sorted_xgb_importance[:10]]\n",
        "top_10_xgb_values   = [item[1] for item in sorted_xgb_importance[:10]]\n",
        "\n",
        "print(\"\\n--- Top 10 Permutation Feature Importance (best_xgb, Time Excluded) ---\")\n",
        "for f, v in zip(top_10_xgb_features, top_10_xgb_values):\n",
        "    print(f\"{f}: {v:.5f}\")\n",
        "\n",
        "\n",
        "#  Plotting top 10 PFI\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    x=top_10_xgb_values,\n",
        "    y=top_10_xgb_features,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.xlabel('Mean Decrease in Score (R²)')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 10 Permutation Feature Importance for XGBoost (Time Excluded)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WiZZ9ZME960C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBOOST with PFI Feature Selection"
      ],
      "metadata": {
        "id": "jNicAlxt_OV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sort PFI results (highest to lowest)\n",
        "sorted_importance = sorted(\n",
        "    xgb_perm_importance.items(),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "# Select Top-10 from PFI\n",
        "top_10_pfi_features = [feat for feat, score in sorted_importance[:10]]\n",
        "\n",
        "# Automatically detect REAL time feature\n",
        "time_cols = [c for c in X_train.columns if \"time\" in c.lower()]\n",
        "if len(time_cols) == 0:\n",
        "    raise ValueError(\"No time column found in X_train!\")\n",
        "time_col = time_cols[0]\n",
        "\n",
        "# Add it if missing\n",
        "if time_col not in top_10_pfi_features:\n",
        "    top_10_pfi_features.append(time_col)\n",
        "\n",
        "print(\"\\nSelected features for final model (Top-10 PFI + Time):\")\n",
        "for f in top_10_pfi_features:\n",
        "    print(\"  -\", f)\n",
        "\n",
        "# Subset train/test to these features\n",
        "X_train_top10 = X_train[top_10_pfi_features].copy()\n",
        "X_test_top10  = X_test[top_10_pfi_features].copy()\n",
        "\n",
        "#Final XGBoost (using best hyperparameters)\n",
        "final_xgb_top10 = XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    tree_method=\"hist\",\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_jobs=-1,\n",
        "    subsample=0.8,\n",
        "    reg_lambda=0.1,\n",
        "    reg_alpha=0,\n",
        "    n_estimators=300,\n",
        "    min_child_weight=1,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.03,\n",
        "    gamma=0.3,\n",
        "    colsample_bytree=0.6\n",
        ")\n",
        "\n",
        "final_xgb_top10.fit(X_train_top10, y_train)\n",
        "\n",
        "# Evaluate final model\n",
        "train_top10_metrics = get_regression_metrics(final_xgb_top10, X_train_top10, y_train)\n",
        "test_top10_metrics  = get_regression_metrics(final_xgb_top10, X_test_top10,  y_test)\n",
        "\n",
        "print(\"\\nFinal XGB (Top-10 PFI + Time) - Train:\", train_top10_metrics)\n",
        "print(\"Final XGB (Top-10 PFI + Time) - Test :\", test_top10_metrics)\n"
      ],
      "metadata": {
        "id": "QLLz8bCJCA1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SHAP Analysis"
      ],
      "metadata": {
        "id": "NIfYBqnekUh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "#Original SHAP values\n",
        "explainer = shap.TreeExplainer(final_xgb_top10)\n",
        "shap_values = explainer(X_train_top10)\n",
        "shap.plots.bar(shap_values)\n"
      ],
      "metadata": {
        "id": "FQtCO8xhvn0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beeswarm Plot"
      ],
      "metadata": {
        "id": "RqEqP73Bcu3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "#Original SHAP values\n",
        "explainer = shap.TreeExplainer(final_xgb_top10)\n",
        "shap_values = explainer(X_train_top10)\n",
        "\n",
        "#Find the time column and its index\n",
        "time_cols = [c for c in X_train_top10.columns if \"time\" in c.lower()]\n",
        "time_col = time_cols[0]          # assume only one time column\n",
        "time_idx = list(X_train_top10.columns).index(time_col)\n",
        "\n",
        "#Build a mask that EXCLUDES the time column\n",
        "mask = np.array([i != time_idx for i in range(len(X_train_top10.columns))])\n",
        "\n",
        "#Create a new SHAP Explanation without time\n",
        "shap_values_no_time = shap.Explanation(\n",
        "    values       = shap_values.values[:, mask],\n",
        "    base_values  = shap_values.base_values,\n",
        "    data         = shap_values.data[:, mask],\n",
        "    feature_names=[name for i, name in enumerate(shap_values.feature_names) if i != time_idx]\n",
        ")\n",
        "\n",
        "#Beeswarm plot WITHOUT time\n",
        "shap.plots.beeswarm(shap_values_no_time)\n"
      ],
      "metadata": {
        "id": "rGRR4lF5u_WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IV. Predicted vs Actual Drug release\n"
      ],
      "metadata": {
        "id": "5IKR5EeAu9WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TEST portion of the original dataset ---\n",
        "test_df = df_with_descriptors.iloc[test_idx].copy()\n",
        "\n",
        "# Identify formulations present in the test set\n",
        "forms_in_test = test_df[\"Formulation Index\"].unique()\n",
        "\n",
        "# Choose 4 representative formulations\n",
        "form_sizes = test_df[\"Formulation Index\"].value_counts()\n",
        "chosen_forms = form_sizes.index[:4].tolist()\n",
        "\n",
        "print(\"Selected unseen formulations:\", chosen_forms)\n",
        "\n",
        "#plot setup\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "time_col = \"Time \"       # adjust if you stripped spaces\n",
        "release_col = \"Release \" # adjust if you stripped spaces\n",
        "\n",
        "for i, form in enumerate(chosen_forms, 1):\n",
        "\n",
        "    # subset data for this formulation\n",
        "    sub_df = test_df[test_df[\"Formulation Index\"] == form].copy()\n",
        "\n",
        "    # ensure time order\n",
        "    sub_df = sub_df.sort_values(by=time_col)\n",
        "\n",
        "    # slice matching rows from X_test\n",
        "    X_sub = X_test.loc[sub_df.index, top_10_pfi_features]\n",
        "\n",
        "    # predictions\n",
        "    sub_df[\"Release_pred\"] = final_xgb_top10.predict(X_sub)\n",
        "\n",
        "    # --- 3. Plot each formulation ---\n",
        "    plt.subplot(2, 2, i)\n",
        "\n",
        "    plt.plot(sub_df[time_col], sub_df[release_col], \"o-\", label=\"Actual\")\n",
        "    plt.plot(sub_df[time_col], sub_df[\"Release_pred\"], \"s--\", label=\"Predicted\")\n",
        "\n",
        "    plt.title(f\"Formulation {form}\")\n",
        "    plt.xlabel(\"Time (days)\")\n",
        "    plt.ylabel(\"Cumulative Release (w/w)\")\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EWQ0XX42XqIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"Formulation Index\"] == 123][[\"Drug\"]]"
      ],
      "metadata": {
        "id": "5rFCMVIyZ734"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V. Drug descriptor and Clustering\n"
      ],
      "metadata": {
        "id": "XXfoi-wfWGo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_descriptors\n",
        "import pandas as pd\n",
        "\n",
        "drug_descriptor_cols = [\n",
        "    \"Drug MW\",\n",
        "    \"Drug TPSA\",\n",
        "    \"Drug LogP\",\n",
        "    # \"Drug pKa\"   # uncomment if exists\n",
        "]\n",
        "\n",
        "# One row per drug\n",
        "drug_df = (\n",
        "    df_with_descriptors\n",
        "    .groupby(\"Drug\")[drug_descriptor_cols]\n",
        "    .mean()        # descriptors should be constant, mean is safe\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "print(drug_df.head())\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_drug = scaler.fit_transform(drug_df[drug_descriptor_cols])"
      ],
      "metadata": {
        "id": "5Go-4cj4WYDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "\n",
        "umap_model = umap.UMAP(\n",
        "    n_neighbors=5,\n",
        "    min_dist=0.3,\n",
        "    n_components=2,\n",
        "    random_state=1147\n",
        ")\n",
        "\n",
        "drug_embedding = umap_model.fit_transform(X_drug)\n",
        "\n",
        "drug_df[\"UMAP1\"] = drug_embedding[:,0]\n",
        "drug_df[\"UMAP2\"] = drug_embedding[:,1]"
      ],
      "metadata": {
        "id": "il_0ZfnYWgkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=3,\n",
        "    metric=\"euclidean\"\n",
        ")\n",
        "\n",
        "drug_df[\"Cluster\"] = clusterer.fit_predict(drug_embedding)\n",
        "\n",
        "print(drug_df[[\"Drug\",\"Cluster\"]])"
      ],
      "metadata": {
        "id": "aFkxoDMaWjS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14,8))\n",
        "\n",
        "scatter = plt.scatter(\n",
        "    drug_df[\"UMAP1\"],\n",
        "    drug_df[\"UMAP2\"],\n",
        "    c=drug_df[\"Cluster\"],\n",
        "    cmap=\"viridis\",\n",
        "    s=80\n",
        ")\n",
        "\n",
        "for i, txt in enumerate(drug_df[\"Drug\"]):\n",
        "    plt.annotate(txt,\n",
        "                 (drug_df[\"UMAP1\"][i], drug_df[\"UMAP2\"][i]),\n",
        "                 fontsize=9)\n",
        "\n",
        "plt.title(\"Drug Descriptor Space (UMAP)\")\n",
        "plt.xlabel(\"UMAP1\")\n",
        "plt.ylabel(\"UMAP2\")\n",
        "plt.colorbar(scatter, label=\"Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FNTbMUbOWmwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_names = {\n",
        "    0: \"Small hydrophobic molecules\",\n",
        "    1: \"Large hydrophilic molecules\",\n",
        "    2: \"Midsized Amphiphilic\",\n",
        "    3: \"Hydrophilic drugs\"\n",
        "}\n",
        "\n",
        "drug_df[\"Cluster_Label\"] = drug_df[\"Cluster\"].map(cluster_names)\n",
        "\n",
        "drug_df[[\"Drug\", \"Cluster\", \"Cluster_Label\"]].head()"
      ],
      "metadata": {
        "id": "ktJ7qODoW9DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build mapping: Drug → Cluster_Label\n",
        "drug_to_cluster = drug_df.set_index(\"Drug\")[\"Cluster_Label\"].to_dict()\n",
        "\n",
        "# Add cluster column to your main dataframe\n",
        "df_with_descriptors[\"Cluster_Label\"] = df_with_descriptors[\"Drug\"].map(drug_to_cluster)"
      ],
      "metadata": {
        "id": "T5VtY0y1XQJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df_with_descriptors.iloc[train_idx].copy()\n",
        "\n",
        "# Keep only rows used in final model\n",
        "train_df = train_df.loc[X_train_top10.index]\n",
        "\n",
        "print(train_df[[\"Drug\",\"Cluster_Label\"]].head())"
      ],
      "metadata": {
        "id": "ow1a0qyFXVem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding which discriptor is important per cluster"
      ],
      "metadata": {
        "id": "fVZ08upzXaSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "\n",
        "# 1) Build train_df from original data, then align to X_train_top10 rows\n",
        "train_df = df_with_descriptors.iloc[train_idx].copy()\n",
        "train_df = train_df.loc[X_train_top10.index].copy()\n",
        "\n",
        "# Sanity check\n",
        "print(train_df[[\"Drug\", \"Cluster_Label\"]].head())\n",
        "print(\"Clusters in train:\", train_df[\"Cluster_Label\"].unique())"
      ],
      "metadata": {
        "id": "eo4duKlRXdhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.TreeExplainer(final_xgb_top10)\n",
        "shap_exp = explainer(X_train_top10)   # shap.Explanation# Identify time column (if present) and remove it from the SHAP matrix\n",
        "feature_names = list(shap_exp.feature_names)\n",
        "\n",
        "time_cols = [c for c in feature_names if \"time\" in c.lower()]\n",
        "if len(time_cols) > 0:\n",
        "    time_col = time_cols[0]\n",
        "    time_idx = feature_names.index(time_col)\n",
        "\n",
        "    keep_mask = np.array([i != time_idx for i in range(len(feature_names))])\n",
        "\n",
        "    shap_values = shap_exp.values[:, keep_mask]\n",
        "    X_vals = X_train_top10.values[:, keep_mask]\n",
        "    kept_features = [f for i, f in enumerate(feature_names) if i != time_idx]\n",
        "else:\n",
        "    time_col = None\n",
        "    shap_values = shap_exp.values\n",
        "    X_vals = X_train_top10.values\n",
        "    kept_features = feature_names\n",
        "\n",
        "print(\"Removed time column:\", time_col)\n",
        "print(\"Features used for cluster-SHAP:\", kept_features)"
      ],
      "metadata": {
        "id": "0otM9jLqYDxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusters = pd.Series(train_df[\"Cluster_Label\"].values, index=X_train_top10.index)\n",
        "\n",
        "cluster_shap = {}\n",
        "\n",
        "for cl in sorted(clusters.dropna().unique()):\n",
        "    idx = (clusters == cl).values\n",
        "    mean_abs = np.abs(shap_values[idx]).mean(axis=0)   # mean |SHAP| for this cluster\n",
        "    cluster_shap[cl] = mean_abs\n",
        "\n",
        "shap_cluster_df = pd.DataFrame(cluster_shap, index=kept_features)\n",
        "\n",
        "# Sort features by overall importance across clusters\n",
        "shap_cluster_df[\"Overall_MeanAbsSHAP\"] = shap_cluster_df.mean(axis=1)\n",
        "shap_cluster_df = shap_cluster_df.sort_values(\"Overall_MeanAbsSHAP\", ascending=False).drop(columns=[\"Overall_MeanAbsSHAP\"])\n",
        "\n",
        "# Display full table\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"\\nMean(|SHAP|) per feature per cluster:\")\n",
        "print(shap_cluster_df.to_string())"
      ],
      "metadata": {
        "id": "I9Nq7fNEYNtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "short_names = {\n",
        "    \"Small hydrophobic molecules\": \"Small\\nHydrophobic\",\n",
        "    \"Large hydrophilic molecules\": \"Large\\nHydrophilic\",\n",
        "    \"Midsized Amphiphilic\": \"Mid\\nAmphiphilic\",\n",
        "    \"Hydrophilic drugs\": \"Hydrophilic\"\n",
        "}\n",
        "\n",
        "plot_df = shap_cluster_df.rename(columns=short_names)\n",
        "\n",
        "plt.figure(figsize=(7.5, 4))\n",
        "ax = sns.heatmap(\n",
        "    plot_df,\n",
        "    cmap=\"viridis\",\n",
        "    linewidths=0.5,\n",
        "    linecolor=\"white\",\n",
        "    cbar_kws={\"label\": \"mean(|SHAP|)\"}\n",
        ")\n",
        "\n",
        "ax.set_title(\"Cluster-wise Feature Importance\", pad=10)\n",
        "ax.set_xlabel(\"\")\n",
        "ax.set_ylabel(\"\")\n",
        "\n",
        "plt.xticks(rotation=0)      # now it fits\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7LR3Hiz_YRQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "short_names = {\n",
        "    \"Small hydrophobic molecules\": \"Small\\nHydrophobic\",\n",
        "    \"Large hydrophilic molecules\": \"Large\\nHydrophilic\",\n",
        "    \"Midsized Amphiphilic\": \"Mid\\nAmphiphilic\",\n",
        "    \"Hydrophilic drugs\": \"Hydrophilic\"\n",
        "}\n",
        "\n",
        "plot_df = shap_cluster_df.copy()\n",
        "\n",
        "# Column-wise normalization (0–1 per cluster)\n",
        "plot_df = (plot_df - plot_df.min()) / (plot_df.max() - plot_df.min() + 1e-12)\n",
        "plot_df = plot_df.rename(columns=short_names)\n",
        "\n",
        "plt.figure(figsize=(7.5, 4))\n",
        "ax = sns.heatmap(\n",
        "    plot_df,\n",
        "    cmap=\"viridis\",\n",
        "    linewidths=0.5,\n",
        "    linecolor=\"white\",\n",
        "    cbar_kws={\"label\": \"Normalized mean(|SHAP|) per cluster\"}\n",
        ")\n",
        "\n",
        "ax.set_title(\"Normalized Cluster-wise Feature Importance\", pad=10)\n",
        "ax.set_xlabel(\"\")\n",
        "ax.set_ylabel(\"\")\n",
        "\n",
        "plt.xticks(rotation=0)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eEjEqIF5Y9Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VI. BAYESIAN OPTIMIZATION depending on Cluster"
      ],
      "metadata": {
        "id": "OEfThl8ol9Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Drug -> Cluster_Label\n",
        "drug_to_cluster = drug_df.set_index(\"Drug\")[\"Cluster_Label\"].to_dict()\n",
        "df_with_descriptors[\"Cluster_Label\"] = df_with_descriptors[\"Drug\"].map(drug_to_cluster)\n",
        "\n",
        "print(df_with_descriptors[\"Cluster_Label\"].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "wXOJcSd1mFPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Criteria of Success from literature of drug release. Large hydrophilic drugs allowed higher burst rate due to its nature.\n",
        "CRITERIA = {\n",
        "    \"Small hydrophobic molecules\": {  # cluster 0\n",
        "        \"burst_max\": 0.20,\n",
        "        \"total_min\": 0.80,\n",
        "    },\n",
        "    \"Large hydrophilic molecules\": {  # cluster 1\n",
        "        \"burst_max\": 0.30,\n",
        "        \"total_min\": 0.70,\n",
        "    },\n",
        "    \"Midsized Amphiphilic\": {         # cluster 2\n",
        "        \"burst_max\": 0.20,\n",
        "        \"total_min\": 0.70,\n",
        "    },\n",
        "    \"Hydrophilic drugs\": {            # cluster 3\n",
        "        \"burst_max\": 0.20,\n",
        "        \"total_min\": 0.70,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Common sustained-release settings\n",
        "DURATION_START_DAY = 14.0\n",
        "DURATION_END_DAY   = 30.0\n",
        "MIN_GAIN_AFTER_14  = 0.10  # at least +10% release after day 14"
      ],
      "metadata": {
        "id": "jUArtke7wxcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = list(X_train_top10.columns)\n",
        "time_cols = [c for c in feature_names if \"time\" in c.lower()]\n",
        "assert len(time_cols) == 1, f\"Expected exactly 1 time feature. Found: {time_cols}\"\n",
        "TIME_FEATURE = time_cols[0]\n",
        "print(\"Time feature =\", TIME_FEATURE)"
      ],
      "metadata": {
        "id": "MLaJHy5OxDXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "# Heuristic: treat these as \"drug descriptors\" to exclude from optimization\n",
        "drug_descriptor_like = set([c for c in df_with_descriptors.columns if c.lower().startswith(\"drug \")])\n",
        "drug_descriptor_like |= set([c for c in feature_names if c.lower().startswith(\"drug \")])\n",
        "\n",
        "# Continuous candidates: numeric model features excluding time and drug descriptors\n",
        "cont_candidates = []\n",
        "for c in feature_names:\n",
        "    if c == TIME_FEATURE:\n",
        "        continue\n",
        "    if c in drug_descriptor_like:\n",
        "        continue\n",
        "    # try to avoid one-hot method dummies here\n",
        "    if \"formulation method_\" in c.lower():\n",
        "        continue\n",
        "    # keep numeric\n",
        "    if c in X_train_top10.columns:\n",
        "        cont_candidates.append(c)\n",
        "\n",
        "# Method dummies (optional)\n",
        "method_dummy_cols = [c for c in feature_names if \"formulation method_\" in c.lower()]\n",
        "\n",
        "print(\"Continuous BO vars (auto):\", cont_candidates)\n",
        "print(\"Method dummy cols:\", method_dummy_cols)"
      ],
      "metadata": {
        "id": "Irb7SR4PxGz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Picking representative drugs in each cluster\n",
        "desc_cols = [c for c in [\"Drug LogP\", \"Drug MW\", \"Drug TPSA\"] if c in drug_df.columns]\n",
        "assert len(desc_cols) >= 2, f\"Need at least 2 drug descriptor columns in drug_df, found: {desc_cols}\"\n",
        "\n",
        "def pick_representative_drug(drug_df, cluster_label):\n",
        "    sub = drug_df[drug_df[\"Cluster_Label\"] == cluster_label].copy()\n",
        "    centroid = sub[desc_cols].mean().values\n",
        "    dists = np.linalg.norm(sub[desc_cols].values - centroid, axis=1)\n",
        "    return sub.iloc[int(np.argmin(dists))][\"Drug\"]\n",
        "\n",
        "clusters = [cl for cl in df_with_descriptors[\"Cluster_Label\"].dropna().unique() if cl in CRITERIA]\n",
        "rep_drug = {cl: pick_representative_drug(drug_df, cl) for cl in clusters}\n",
        "rep_drug"
      ],
      "metadata": {
        "id": "Jl1cbPLNxPAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Align training rows to X_train_top10 index (if you use train_idx elsewhere, you can swap here)\n",
        "train_df_aligned = df_with_descriptors.loc[X_train_top10.index].copy()\n",
        "\n",
        "def get_cluster_bounds(X, train_df, cluster_label, vars_cont):\n",
        "    sub_idx = train_df[\"Cluster_Label\"] == cluster_label\n",
        "    bounds = {}\n",
        "    for v in vars_cont:\n",
        "        lo = float(X.loc[sub_idx, v].min())\n",
        "        hi = float(X.loc[sub_idx, v].max())\n",
        "        if not np.isfinite(lo) or not np.isfinite(hi) or lo == hi:\n",
        "            # fallback: global\n",
        "            lo = float(X[v].min()); hi = float(X[v].max())\n",
        "        bounds[v] = (lo, hi)\n",
        "    return bounds\n",
        "\n",
        "cluster_bounds = {cl: get_cluster_bounds(X_train_top10, train_df_aligned, cl, cont_candidates) for cl in clusters}\n",
        "cluster_bounds"
      ],
      "metadata": {
        "id": "-G0_oZpIxUyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_curve(pred_curve, times, cluster_label):\n",
        "    cfg = CRITERIA[cluster_label]\n",
        "\n",
        "    # Interpolate burst and day14\n",
        "    r_day1  = float(np.interp(1.0, times, pred_curve))\n",
        "    r_day14 = float(np.interp(DURATION_START_DAY, times, pred_curve))\n",
        "    r_final = float(pred_curve[-1])\n",
        "\n",
        "    loss = 0.0\n",
        "\n",
        "    # 1) Burst (soft constraint)\n",
        "    burst_excess = max(0.0, r_day1 - cfg[\"burst_max\"])\n",
        "    loss += 25.0 * (burst_excess ** 2)\n",
        "\n",
        "    # Preder ~0.15 for clusters with 0.20 max (<15–20%)\n",
        "    if cfg[\"burst_max\"] <= 0.20:\n",
        "        loss += 2.0 * ((r_day1 - 0.15) ** 2)\n",
        "\n",
        "    # 2) Total release at end\n",
        "    total_deficit = max(0.0, cfg[\"total_min\"] - r_final)\n",
        "    loss += 40.0 * (total_deficit ** 2)\n",
        "\n",
        "    # 3) Monotonicity\n",
        "    diffs = np.diff(pred_curve)\n",
        "    neg = diffs[diffs < 0]\n",
        "    if len(neg):\n",
        "        loss += 80.0 * float(np.sum(neg**2))\n",
        "\n",
        "    # 4) Sustained duration proxy: must keep releasing after day 14\n",
        "    gain_after_14 = r_final - r_day14\n",
        "    duration_deficit = max(0.0, MIN_GAIN_AFTER_14 - gain_after_14)\n",
        "    loss += 15.0 * (duration_deficit ** 2)\n",
        "\n",
        "    return float(loss)"
      ],
      "metadata": {
        "id": "BK12pWzWyJXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_row_for_drug(df, drug_name):\n",
        "    sub = df[df[\"Drug\"] == drug_name]\n",
        "    assert len(sub) > 0, f\"No rows for drug: {drug_name}\"\n",
        "    return sub.iloc[0]\n",
        "\n",
        "def set_method_dummies(row_dict, method_cols, chosen=None):\n",
        "    # chosen=None means baseline (all zeros)\n",
        "    for c in method_cols:\n",
        "        row_dict[c] = 0.0\n",
        "    if chosen is not None:\n",
        "        row_dict[chosen] = 1.0\n",
        "\n",
        "def predict_curve_xgb(model, base_row, params_cont, times, method_cols=None, method_choice=None):\n",
        "    rows = []\n",
        "    for t in times:\n",
        "        r = {c: 0.0 for c in X_train_top10.columns}\n",
        "\n",
        "        # Fill any features that exist in the base row (drug descriptors etc.)\n",
        "        for c in X_train_top10.columns:\n",
        "            if c in base_row.index:\n",
        "                val = base_row[c]\n",
        "                if pd.notnull(val):\n",
        "                    r[c] = float(val)\n",
        "\n",
        "        # Apply continuous design knobs\n",
        "        for k, v in params_cont.items():\n",
        "            if k in r:\n",
        "                r[k] = float(v)\n",
        "\n",
        "        # Apply method (if dummies exist)\n",
        "        if method_cols and len(method_cols):\n",
        "            set_method_dummies(r, method_cols, method_choice)\n",
        "\n",
        "        # Set time feature\n",
        "        r[TIME_FEATURE] = float(t)\n",
        "\n",
        "        rows.append(r)\n",
        "\n",
        "    Xq = pd.DataFrame(rows, columns=X_train_top10.columns)\n",
        "    preds = model.predict(Xq)\n",
        "    return np.clip(preds, 0.0, 1.0)"
      ],
      "metadata": {
        "id": "-cM-T4TxyXXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VI. a. Bayesian Optimization per cluster"
      ],
      "metadata": {
        "id": "iSCPRvkeyfZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install scikit-optimize"
      ],
      "metadata": {
        "id": "UxyOhmM46aOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "def run_bo_for_cluster(cluster_label, n_calls=50, n_init=12, seed=1147):\n",
        "    # Representative drug\n",
        "    drug_name = rep_drug[cluster_label]\n",
        "    base_row = get_base_row_for_drug(df_with_descriptors, drug_name)\n",
        "\n",
        "    # Bounds\n",
        "    bounds = cluster_bounds[cluster_label]\n",
        "\n",
        "    # Search space: continuous\n",
        "    space = [Real(bounds[v][0], bounds[v][1], name=v) for v in cont_candidates]\n",
        "\n",
        "    # Search space: method dummy choice (optional)\n",
        "    method_choices = [None] + method_dummy_cols  # None = baseline (all zeros)\n",
        "    if len(method_dummy_cols):\n",
        "        space.append(Categorical(method_choices, name=\"MethodDummy\"))\n",
        "\n",
        "    @use_named_args(space)\n",
        "    def objective(**kwargs):\n",
        "        # Split params\n",
        "        params_cont = {v: kwargs[v] for v in cont_candidates}\n",
        "        method_choice = kwargs.get(\"MethodDummy\", None)\n",
        "\n",
        "        pred_curve = predict_curve_xgb(\n",
        "            model=final_xgb_top10,\n",
        "            base_row=base_row,\n",
        "            params_cont=params_cont,\n",
        "            times=TIMES,\n",
        "            method_cols=method_dummy_cols,\n",
        "            method_choice=method_choice\n",
        "        )\n",
        "\n",
        "        return score_curve(pred_curve, TIMES, cluster_label)\n",
        "\n",
        "    res = gp_minimize(\n",
        "        objective,\n",
        "        dimensions=space,\n",
        "        n_calls=n_calls,\n",
        "        n_initial_points=n_init,\n",
        "        random_state=seed,\n",
        "        acq_func=\"EI\"\n",
        "    )\n",
        "\n",
        "    # Decode best\n",
        "    best = {dim.name: val for dim, val in zip(space, res.x)}\n",
        "    best_cont = {v: best[v] for v in cont_candidates}\n",
        "    best_method = best.get(\"MethodDummy\", None)\n",
        "\n",
        "    best_curve = predict_curve_xgb(\n",
        "        model=final_xgb_top10,\n",
        "        base_row=base_row,\n",
        "        params_cont=best_cont,\n",
        "        times=TIMES,\n",
        "        method_cols=method_dummy_cols,\n",
        "        method_choice=best_method\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"cluster\": cluster_label,\n",
        "        \"representative_drug\": drug_name,\n",
        "        \"best_loss\": float(res.fun),\n",
        "        \"best_continuous_params\": best_cont,\n",
        "        \"best_method_dummy\": best_method,\n",
        "        \"times\": TIMES,\n",
        "        \"best_pred_curve\": best_curve,\n",
        "        \"skopt_result\": res\n",
        "    }\n",
        "\n",
        "bo_results = []\n",
        "for cl in clusters:\n",
        "    print(f\"\\n=== BO for cluster: {cl} | rep drug: {rep_drug[cl]} ===\")\n",
        "    bo_results.append(run_bo_for_cluster(cl, n_calls=50, n_init=12))\n",
        "\n",
        "bo_results[0].keys()"
      ],
      "metadata": {
        "id": "i-YALdoQybXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VI.b. Best Curve per cluster and reccomendation"
      ],
      "metadata": {
        "id": "uYB413ZEykpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "for r in bo_results:\n",
        "    plt.plot(r[\"times\"], r[\"best_pred_curve\"], marker=\"o\", label=r[\"cluster\"])\n",
        "plt.xlabel(\"Time (days)\")\n",
        "plt.ylabel(\"Predicted cumulative release\")\n",
        "plt.title(\"BO-Optimized Sustained Release Curves (per cluster)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "for r in bo_results:\n",
        "    print(\"\\n---\", r[\"cluster\"], \"---\")\n",
        "    print(\"Representative drug:\", r[\"representative_drug\"])\n",
        "    print(\"Best loss:\", r[\"best_loss\"])\n",
        "    print(\"Best continuous params:\")\n",
        "    for k, v in r[\"best_continuous_params\"].items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "    if len(method_dummy_cols):\n",
        "        print(\"Best method dummy:\", r[\"best_method_dummy\"])"
      ],
      "metadata": {
        "id": "2TEtInphyj6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}